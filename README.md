# README

## Обзор проекта

Этот проект создан для автоматизированного тестирования и сравнения нейросетевых моделей, установленных локально, с использованием платформы Ollama. Основная цель — оценить качество генерации JUnit тестов на основе анализа Java-кода. Проект позволяет:

- Генерировать промты из Java-кода и соответствующих текстовых описаний.
- Отправлять промты выбранной модели (например, Lama3.3, Deepseek-r1, CodeLlama) через Ollama.
- Принимать ответы от моделей и сохранять их в виде Java тестовых классов.
- В дальнейшем планируется оценивать модели по множеству метрик: покрытие кода (coverage), стабильность, время отклика, потребление ресурсов (CPU, GPU, RAM), длина сгенерированного ответа, а также корректность и качество сгенерированных тестов.

## Архитектура проекта

### 1. Создание промтов для тестирования Java-кода
**Скрипт:** `CreatePromtFromCodeAndAnnotation.py`

- Рекурсивно проходит по директории с Java-кодом.
- Для каждого Java-класса ищет одноимённый текстовый файл с описанием.
- Формирует промт по шаблону:
  
  ```
  Задача: Проанализируй следующий Java-код. Напиши на него JUnit тесты. Каждый тест должен иметь название <ИмяМетода>_Where<условие>_Then<ожидание>. Ответом должен быть просто Java Test class без лишнего текста.
  
  Описание кода: <содержимое текстового файла с описанием>
  
  Код:
  
  --- Начало кода ---
  <содержимое Java файла>
  
  --- Конец кода ---
  ```
  
- Сохраняет каждый промт в отдельный файл в папке `promts/notOptimized` с именем `<ИмяJavaКласса>_Promt.txt`.

### 2. Коммуникация с Ollama и отправка промтов
**Скрипт:** `OllamaCommunicationScript.py`

- Предоставляет список доступных моделей (Lama3.3, Deepseek-r1, CodeLlama) и позволяет выбрать модель.
- Читает промты из папки `promts/notOptimized`.
- Отправляет каждый промт выбранной модели через команду `ollama run`.
- Сохраняет ответы моделей в папке `response/<model_name>` с именем файла, основанном на имени промта.

### 3. Генерация Java тестовых классов из ответа модели
**Скрипт:** `CreateJavaTestClassFromAiResponse.py`

- Читает содержимое файла ответа из `response/<model_name>`.
- Извлекает имя тестового класса из строки вида `public class <ИмяКласса>`.
- Сохраняет ответ в виде Java файла `<ИмяКласса>.java` в директории `generated/tests`.

## Генерация тестов и оценка моделей

### Генерация тестов

На основе отправленного промта модели генерируют JUnit тестовый класс. Тесты должны называться по схеме `<ИмяМетода>_Where<условие>_Then<ожидание>`, что позволяет сразу понять, что именно тестируется.

### Оценка моделей

В дальнейшем планируется интеграция следующих компонентов для оценки:

- **CountStatistic:** модуль для сбора статистики по сгенерированным тестам (количество тестов, покрытие кода, повторяемость выполнения).
- **TryCompileTests:** модуль для компиляции сгенерированных тестовых классов, проверки их работоспособности и запуска unit-тестов.
- **Сбор метрик:** время отклика, потребление системных ресурсов (CPU, GPU, RAM), длина ответа модели и качество сгенерированного кода.
- Результаты будут сохраняться в CSV-файлах и визуализироваться в Jupyter Notebook для детального сравнения.

## Установка и настройка

### Требования:
- Python 3.x
- Доступ к Ollama для отправки запросов к выбранным моделям
- Java (для компиляции сгенерированных тестовых классов)
- Установленные библиотеки: `os`, `sys`, `subprocess`, `glob`, `pathlib`, `re`

### Структура каталогов:

```
javaProjects/ToPromtCode/main/java/backend/academy/bot/  # Исходный Java-код и текстовые описания для промтов
promts/notOptimized                                      # Сгенерированные файлы промтов
response/<model_name>                                    # Папки, куда сохраняются ответы моделей
generated/tests                                          # Папка для сохранения сгенерированных Java тестовых классов
```

### Установка зависимостей:
Зависимости для Python можно установить через `pip`, если используются сторонние библиотеки (например, для парсинга аргументов или работы с файлами). В текущей реализации используются только стандартные модули Python.

## Запуск проекта

### 1. Генерация промтов
```bash
python CreatePromtFromCodeAndAnnotation.py
```
Этот скрипт прочитает Java-код и соответствующие текстовые описания, после чего сгенерирует файлы промтов.

### 2. Отправка промтов модели через Ollama
```bash
python OllamaCommunicationScript.py
```
Выберите нужную модель и отправьте промты. Ответы сохранятся в папке `response/<model_name>`.

### 3. Генерация Java тестового класса
```bash
python CreateJavaTestClassFromAiResponse.py
```
Скрипт прочитает ответ модели, извлечет имя тестового класса и сохранит файл в `generated/tests`.

## Будущие доработки

- **CountStatistic:**
  Планируется разработка класса для сбора и анализа статистики по сгенерированным тестам (количество тестов, покрытие, стабильность, повторяемость).

- **TryCompileTests:**
  Будет реализована функциональность для компиляции сгенерированных тестовых классов и проверки их работоспособности с помощью unit-тестов.

- **Автоматизация и CI/CD:**
  Настройка автоматического запуска тестов и сбора метрик при изменениях в коде, что позволит ускорить оценку качества моделей.

- **Визуализация данных:**
  Создание Jupyter Notebook для построения графиков и сводных таблиц, позволяющих сравнивать модели по ключевым метрикам.

## Итог

Проект направлен на сравнение локально установленных нейросетей по качеству генерации JUnit тестов для Java-кода. Используя Ollama, мы отправляем стандартизированные промты моделям, получаем ответы и автоматически преобразуем их в тестовые классы. В дальнейшем планируется интеграция модулей для анализа и компиляции тестов, что позволит комплексно оценивать модели по множеству метрик, таких как покрытие, стабильность, время отклика и потребление ресурсов.

Если возникнут вопросы или предложения по улучшению проекта, пожалуйста, свяжитесь с разработчиками или откройте issue в репозитории.

